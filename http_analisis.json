{"paragraphs":[{"text":"%md\n## Analise de logs HTTP com PySpark\n\nAutor: Bruno F. Bessa","user":"anonymous","dateUpdated":"2018-05-29T01:56:31+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1527558949021_256570353","id":"20180529-015549_955434701","dateCreated":"2018-05-29T01:55:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4197","dateFinished":"2018-05-29T01:56:31+0000","dateStarted":"2018-05-29T01:56:31+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Analise de logs HTTP com PySpark</h2>\n<p>Autor: Bruno F. Bessa</p>\n</div>"}]}},{"text":"%spark.pyspark\n# Importacao de pacotes necessarios:\n\nimport re\nimport pyspark.sql.functions\nfrom pyspark.sql.types import *\n","user":"anonymous","dateUpdated":"2018-05-29T04:28:21+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1527558945411_490482360","id":"20180529-015545_58623231","dateCreated":"2018-05-29T01:55:45+0000","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4138","dateFinished":"2018-05-29T04:27:47+0000","dateStarted":"2018-05-29T04:27:47+0000","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-686110285505571523.py\", line 344, in <module>\n    code = compile('\\n'.join(final_code), '<stdin>', 'exec', ast.PyCF_ONLY_AST, 1)\n  File \"<stdin>\", line 4\n    import org.apache.spark.sql.functions.{dayofmonth,from_unixtime,month, unix_timestamp, year}\n                                          ^\nSyntaxError: invalid syntax\n"}]}},{"text":"%spark.pyspark\n# Assegurando-se de que ha uma conexao com Spark:\n\nsc","user":"anonymous","dateUpdated":"2018-05-29T03:03:39+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"<SparkContext master=local[*] appName=Zeppelin>\n"}]},"apps":[],"jobName":"paragraph_1527551158267_-1913766947","id":"20180528-234558_1244763647","dateCreated":"2018-05-28T23:45:58+0000","dateStarted":"2018-05-29T03:03:39+0000","dateFinished":"2018-05-29T03:03:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3724"},{"text":"%spark.pyspark\n# Carga dos arquivos de requisicoes HTTP via Spark Context na forma de RDD.\n\ntextFile1 = sc.textFile(\"NASA_access_log_Jul95\")\ntextFile2 = sc.textFile(\"NASA_access_log_Ago95\")\n\nlogLines = textFile1.union(textFile2)\nlogLines.cache()","user":"anonymous","dateUpdated":"2018-05-29T04:34:43+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"UnionRDD[321] at union at NativeMethodAccessorImpl.java:0\n"}]},"apps":[],"jobName":"paragraph_1527551167001_-810090919","id":"20180528-234607_1213421713","dateCreated":"2018-05-28T23:46:07+0000","dateStarted":"2018-05-29T04:34:43+0000","dateFinished":"2018-05-29T04:34:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3725","focus":true},{"text":"%md \nA descricao dos registros dos logs e a seguinte:\n\n* host responsavel pela requisicao\n* timestamo da data\n* requisicao\n* codigo de retorno HTTP\n* total de bytes retornados\n\nCom estas informacoes, farei a quebra em colunas do arquivo de texto de uma forma mais intuitiva do que utilizando expressoes regulares. Vemos que o formato padrao dos registros segue a forma abaixo:\n\n'199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245'\n\nEntao usarei alguns dos caracteres que delimitam os campos para separa-los.","user":"anonymous","dateUpdated":"2018-05-29T03:15:52+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>A descricao dos registros dos logs e a seguinte:</p>\n<ul>\n  <li>host responsavel pela requisicao</li>\n  <li>timestamo da data</li>\n  <li>requisicao</li>\n  <li>codigo de retorno HTTP</li>\n  <li>total de bytes retornados</li>\n</ul>\n<p>Com estas informacoes, farei a quebra em colunas do arquivo de texto de uma forma mais intuitiva do que utilizando expressoes regulares. Vemos que o formato padrao dos registros segue a forma abaixo:</p>\n<p>&lsquo;199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] &ldquo;GET /history/apollo/ HTTP/1.0&rdquo; 200 6245&rsquo;</p>\n<p>Entao usarei alguns dos caracteres que delimitam os campos para separa-los.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1527551173972_-576148263","id":"20180528-234613_1535656301","dateCreated":"2018-05-28T23:46:13+0000","dateStarted":"2018-05-29T03:15:52+0000","dateFinished":"2018-05-29T03:15:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3726"},{"text":"%spark.pyspark\n# Identificando alguns caracteres que podem ser substituidos por um unico identificador.\ntemp_var = logLines.map(lambda k: k.replace(\" - - [\", \";\"))\ntemp_var2 = temp_var.map(lambda k: k.replace('] \"', \";\"))\ntemp_var3 = temp_var2.map(lambda k: k.replace('\" ', \";\"))\n\n# Os dois ultimos campos podem ser separados posteriormente pois têáem o formato mais simples. \n# Com o caracter ; podemos separar as variaveis e criar um data frame.\ntemp_var4 = temp_var3.map(lambda k: k.split(\";\"))\nlogLinesDF = temp_var4.toDF()\nlogLinesDF.show(2)","user":"anonymous","dateUpdated":"2018-05-29T04:38:35+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1527559496405_-1861151655","id":"20180529-020456_1348121547","dateCreated":"2018-05-29T02:04:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4445","dateFinished":"2018-05-29T04:38:35+0000","dateStarted":"2018-05-29T04:38:35+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+--------------------+--------------------+--------+\n|                  _1|                  _2|                  _3|      _4|\n+--------------------+--------------------+--------------------+--------+\n|        199.72.81.55|01/Jul/1995:00:00...|GET /history/apol...|200 6245|\n|unicomp6.unicomp.net|01/Jul/1995:00:00...|GET /shuttle/coun...|200 3985|\n+--------------------+--------------------+--------------------+--------+\nonly showing top 2 rows\n\n"}]}},{"text":"%spark.pyspark\n# Tratando a separacao das duas ultimas colunas:\nsplit_col = pyspark.sql.functions.split(logLinesDF[\"_4\"], \" \")\nlogLinesDF = logLinesDF.withColumn(\"codigo_http\", split_col.getItem(0))\nlogLinesDF = logLinesDF.withColumn(\"total_bytes\", split_col.getItem(1))\n\n# Da mesma forma, removerei o timezone da data-hora:\nsplit_col = pyspark.sql.functions.split(logLinesDF[\"_2\"], \" -\")\nlogLinesDF = logLinesDF.withColumn(\"data_hora_string\", split_col.getItem(0))\nlogLinesDF = logLinesDF.withColumn(\"timezone\", split_col.getItem(1))\n\nlogLinesDF.show()","user":"anonymous","dateUpdated":"2018-05-29T04:38:38+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1527562705962_-1147263489","id":"20180529-025825_1164224911","dateCreated":"2018-05-29T02:58:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6633","dateFinished":"2018-05-29T04:38:38+0000","dateStarted":"2018-05-29T04:38:38+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+--------------------+--------------------+---------+-----------+-----------+--------------------+--------+\n|                  _1|                  _2|                  _3|       _4|codigo_http|total_bytes|    data_hora_string|timezone|\n+--------------------+--------------------+--------------------+---------+-----------+-----------+--------------------+--------+\n|        199.72.81.55|01/Jul/1995:00:00...|GET /history/apol...| 200 6245|        200|       6245|01/Jul/1995:00:00:01|    0400|\n|unicomp6.unicomp.net|01/Jul/1995:00:00...|GET /shuttle/coun...| 200 3985|        200|       3985|01/Jul/1995:00:00:06|    0400|\n|      199.120.110.21|01/Jul/1995:00:00...|GET /shuttle/miss...| 200 4085|        200|       4085|01/Jul/1995:00:00:09|    0400|\n|  burger.letters.com|01/Jul/1995:00:00...|GET /shuttle/coun...|    304 0|        304|          0|01/Jul/1995:00:00:11|    0400|\n|      199.120.110.21|01/Jul/1995:00:00...|GET /shuttle/miss...| 200 4179|        200|       4179|01/Jul/1995:00:00:11|    0400|\n|  burger.letters.com|01/Jul/1995:00:00...|GET /images/NASA-...|    304 0|        304|          0|01/Jul/1995:00:00:12|    0400|\n|  burger.letters.com|01/Jul/1995:00:00...|GET /shuttle/coun...|    200 0|        200|          0|01/Jul/1995:00:00:12|    0400|\n|     205.212.115.106|01/Jul/1995:00:00...|GET /shuttle/coun...| 200 3985|        200|       3985|01/Jul/1995:00:00:12|    0400|\n|         d104.aa.net|01/Jul/1995:00:00...|GET /shuttle/coun...| 200 3985|        200|       3985|01/Jul/1995:00:00:13|    0400|\n|      129.94.144.152|01/Jul/1995:00:00...|      GET / HTTP/1.0| 200 7074|        200|       7074|01/Jul/1995:00:00:13|    0400|\n|unicomp6.unicomp.net|01/Jul/1995:00:00...|GET /shuttle/coun...|200 40310|        200|      40310|01/Jul/1995:00:00:14|    0400|\n|unicomp6.unicomp.net|01/Jul/1995:00:00...|GET /images/NASA-...|  200 786|        200|        786|01/Jul/1995:00:00:14|    0400|\n|unicomp6.unicomp.net|01/Jul/1995:00:00...|GET /images/KSC-l...| 200 1204|        200|       1204|01/Jul/1995:00:00:14|    0400|\n|         d104.aa.net|01/Jul/1995:00:00...|GET /shuttle/coun...|200 40310|        200|      40310|01/Jul/1995:00:00:15|    0400|\n|         d104.aa.net|01/Jul/1995:00:00...|GET /images/NASA-...|  200 786|        200|        786|01/Jul/1995:00:00:15|    0400|\n|         d104.aa.net|01/Jul/1995:00:00...|GET /images/KSC-l...| 200 1204|        200|       1204|01/Jul/1995:00:00:15|    0400|\n|      129.94.144.152|01/Jul/1995:00:00...|GET /images/ksclo...|    304 0|        304|          0|01/Jul/1995:00:00:17|    0400|\n|      199.120.110.21|01/Jul/1995:00:00...|GET /images/launc...| 200 1713|        200|       1713|01/Jul/1995:00:00:17|    0400|\n|ppptky391.asahi-n...|01/Jul/1995:00:00...|GET /facts/about_...| 200 3977|        200|       3977|01/Jul/1995:00:00:18|    0400|\n|  net-1-141.eden.com|01/Jul/1995:00:00...|GET /shuttle/miss...|200 34029|        200|      34029|01/Jul/1995:00:00:19|    0400|\n+--------------------+--------------------+--------------------+---------+-----------+-----------+--------------------+--------+\nonly showing top 20 rows\n\n"}]}},{"text":"%spark.pyspark\n# Renomeando as colunas do dataframe\nlogLinesDF = logLinesDF.select(pyspark.sql.functions.col(\"_1\").alias(\"host\"), \n                                pyspark.sql.functions.col(\"data_hora_string\").substr(1, 11).alias(\"data_string\"),\n                                pyspark.sql.functions.col(\"_3\").alias(\"requisicao\"),\n                                pyspark.sql.functions.col(\"codigo_http\").alias(\"codigo_http\"),\n                                pyspark.sql.functions.col(\"codigo_http\").alias(\"total_bytes\"))\n\nlogLinesDF.show()\n\n","user":"anonymous","dateUpdated":"2018-05-29T04:38:57+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1527563087154_-1538970162","id":"20180529-030447_747793605","dateCreated":"2018-05-29T03:04:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6810","dateFinished":"2018-05-29T04:38:43+0000","dateStarted":"2018-05-29T04:38:43+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+-----------+--------------------+-----------+-----------+\n|                host|data_string|          requisicao|codigo_http|total_bytes|\n+--------------------+-----------+--------------------+-----------+-----------+\n|        199.72.81.55|01/Jul/1995|GET /history/apol...|        200|        200|\n|unicomp6.unicomp.net|01/Jul/1995|GET /shuttle/coun...|        200|        200|\n|      199.120.110.21|01/Jul/1995|GET /shuttle/miss...|        200|        200|\n|  burger.letters.com|01/Jul/1995|GET /shuttle/coun...|        304|        304|\n|      199.120.110.21|01/Jul/1995|GET /shuttle/miss...|        200|        200|\n|  burger.letters.com|01/Jul/1995|GET /images/NASA-...|        304|        304|\n|  burger.letters.com|01/Jul/1995|GET /shuttle/coun...|        200|        200|\n|     205.212.115.106|01/Jul/1995|GET /shuttle/coun...|        200|        200|\n|         d104.aa.net|01/Jul/1995|GET /shuttle/coun...|        200|        200|\n|      129.94.144.152|01/Jul/1995|      GET / HTTP/1.0|        200|        200|\n|unicomp6.unicomp.net|01/Jul/1995|GET /shuttle/coun...|        200|        200|\n|unicomp6.unicomp.net|01/Jul/1995|GET /images/NASA-...|        200|        200|\n|unicomp6.unicomp.net|01/Jul/1995|GET /images/KSC-l...|        200|        200|\n|         d104.aa.net|01/Jul/1995|GET /shuttle/coun...|        200|        200|\n|         d104.aa.net|01/Jul/1995|GET /images/NASA-...|        200|        200|\n|         d104.aa.net|01/Jul/1995|GET /images/KSC-l...|        200|        200|\n|      129.94.144.152|01/Jul/1995|GET /images/ksclo...|        304|        304|\n|      199.120.110.21|01/Jul/1995|GET /images/launc...|        200|        200|\n|ppptky391.asahi-n...|01/Jul/1995|GET /facts/about_...|        200|        200|\n|  net-1-141.eden.com|01/Jul/1995|GET /shuttle/miss...|        200|        200|\n+--------------------+-----------+--------------------+-----------+-----------+\nonly showing top 20 rows\n\n"}]}},{"text":"%spark.pyspark\n# Resolvendo os problemas propostos no desafio:\n\nlogLinesDF.select(\"host\").distinct().count()\n\n\n","user":"anonymous","dateUpdated":"2018-05-29T04:47:50+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1527551198404_-1497206482","id":"20180528-234638_1331803448","dateCreated":"2018-05-28T23:46:38+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:3730","dateFinished":"2018-05-29T04:47:53+0000","dateStarted":"2018-05-29T04:47:50+0000","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-686110285505571523.py\", line 360, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/spark-2.2.0/python/pyspark/sql/dataframe.py\", line 427, in count\n    return int(self._jdf.count())\n  File \"/usr/local/lib/python3.4/dist-packages/py4j-0.10.6-py3.4.egg/py4j/java_gateway.py\", line 1160, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/spark-2.2.0/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/local/lib/python3.4/dist-packages/py4j-0.10.6-py3.4.egg/py4j/protocol.py\", line 320, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o3169.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 133.0 failed 1 times, most recent failure: Lost task 0.0 in stage 133.0 (TID 139, localhost, executor driver): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 4 fields are required while 5 values are provided.\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.fromJava(EvaluatePython.scala:138)\n\tat org.apache.spark.sql.SparkSession$$anonfun$5.apply(SparkSession.scala:728)\n\tat org.apache.spark.sql.SparkSession$$anonfun$5.apply(SparkSession.scala:728)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.next(InMemoryRelation.scala:100)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.next(InMemoryRelation.scala:92)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:278)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2430)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2429)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2429)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 4 fields are required while 5 values are provided.\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.fromJava(EvaluatePython.scala:138)\n\tat org.apache.spark.sql.SparkSession$$anonfun$5.apply(SparkSession.scala:728)\n\tat org.apache.spark.sql.SparkSession$$anonfun$5.apply(SparkSession.scala:728)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.next(InMemoryRelation.scala:100)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.next(InMemoryRelation.scala:92)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-686110285505571523.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-686110285505571523.py\", line 360, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/spark-2.2.0/python/pyspark/sql/dataframe.py\", line 427, in count\n    return int(self._jdf.count())\n  File \"/usr/local/lib/python3.4/dist-packages/py4j-0.10.6-py3.4.egg/py4j/java_gateway.py\", line 1160, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/spark-2.2.0/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/local/lib/python3.4/dist-packages/py4j-0.10.6-py3.4.egg/py4j/protocol.py\", line 320, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o3169.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 133.0 failed 1 times, most recent failure: Lost task 0.0 in stage 133.0 (TID 139, localhost, executor driver): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 4 fields are required while 5 values are provided.\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.fromJava(EvaluatePython.scala:138)\n\tat org.apache.spark.sql.SparkSession$$anonfun$5.apply(SparkSession.scala:728)\n\tat org.apache.spark.sql.SparkSession$$anonfun$5.apply(SparkSession.scala:728)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.next(InMemoryRelation.scala:100)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.next(InMemoryRelation.scala:92)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:278)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2430)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2429)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2429)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 4 fields are required while 5 values are provided.\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.fromJava(EvaluatePython.scala:138)\n\tat org.apache.spark.sql.SparkSession$$anonfun$5.apply(SparkSession.scala:728)\n\tat org.apache.spark.sql.SparkSession$$anonfun$5.apply(SparkSession.scala:728)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.next(InMemoryRelation.scala:100)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.next(InMemoryRelation.scala:92)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n\n\n"}]}},{"text":"%spark.pyspark\nlogLinesDF.cache()","user":"anonymous","dateUpdated":"2018-05-29T04:46:27+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1527566370383_-1494954270","id":"20180529-035930_878380523","dateCreated":"2018-05-29T03:59:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9096","dateFinished":"2018-05-29T04:46:27+0000","dateStarted":"2018-05-29T04:46:27+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"DataFrame[host: string, data_string: string, requisicao: string, codigo_http: string, total_bytes: string]\n"}]}},{"text":"%spark.pyspark\n\n","user":"anonymous","dateUpdated":"2018-05-29T04:37:30+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1527566738151_1447369999","id":"20180529-040538_1246924898","dateCreated":"2018-05-29T04:05:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9436","dateFinished":"2018-05-29T04:28:59+0000","dateStarted":"2018-05-29T04:28:59+0000","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-686110285505571523.py\", line 360, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\nTypeError: 'DataFrame' object is not callable\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-686110285505571523.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-686110285505571523.py\", line 360, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\nTypeError: 'DataFrame' object is not callable\n\n"}]}},{"text":"%spark.pyspark\n","user":"anonymous","dateUpdated":"2018-05-29T04:30:42+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1527566771363_444191699","id":"20180529-040611_1641213248","dateCreated":"2018-05-29T04:06:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9522","dateFinished":"2018-05-29T04:30:42+0000","dateStarted":"2018-05-29T04:30:42+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%spark.pyspark\n","user":"anonymous","dateUpdated":"2018-05-29T04:30:48+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1527568242404_-1244616840","id":"20180529-043042_1815082767","dateCreated":"2018-05-29T04:30:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:10642","dateFinished":"2018-05-29T04:30:49+0000","dateStarted":"2018-05-29T04:30:48+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----+\n|  dt|\n+----+\n|null|\n|null|\n|null|\n|null|\n|null|\n|null|\n|null|\n|null|\n|null|\n|null|\n|null|\n|null|\n|null|\n|null|\n|null|\n|null|\n|null|\n|null|\n|null|\n|null|\n+----+\nonly showing top 20 rows\n\n"}]}},{"text":"%spark.pyspark\n","user":"anonymous","dateUpdated":"2018-05-29T04:30:48+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1527568248780_-781748590","id":"20180529-043048_328735103","dateCreated":"2018-05-29T04:30:48+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:10698"}],"name":"http_analysis","id":"2DDUZSK1U","angularObjects":{"2DFP3DPWY:shared_process":[],"2DE4569GT:shared_process":[],"2DGX8KE2S:shared_process":[],"2DF5VR3E9:shared_process":[],"2DGCGF1KR:shared_process":[],"2DGDAG2R8:shared_process":[],"2DH8EJHZF:shared_process":[],"2DDWB5SPQ:shared_process":[],"2DEX6V6KS:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}